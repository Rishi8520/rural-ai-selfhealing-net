# --- File: calculation_agent.py ---
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler
from collections import deque
import shap
import asyncio
import zmq.asyncio
import json
import time
import sys
from functools import partial
import os # For creating directories and saving plots
import matplotlib.pyplot as plt # For saving SHAP plots

# Import the SalpSwarmOptimizer from its dedicated file
from salp_swarm_optimizer import SalpSwarmOptimizer

# --- Path to your dataset ---
# IMPORTANT: This must point to the training_data.csv generated by generate_synthetic_dataset.py
DATASET_PATH = "synthetic_network_data/training_data.csv"

# --- LSTM Model for Anomaly Detection ---
# This model is designed for reconstruction-based anomaly detection.
# It predicts the next sequence of features given the current sequence.
# Anomaly is detected by comparing the prediction with the actual next sequence (reconstruction error).
class LSTMAnomalyModel(nn.Module):
    def __init__(self, input_size, hidden_size, num_layers=2, dropout_rate=0.2):
        """
        Initializes the LSTM Anomaly Model.

        Args:
            input_size (int): The number of features in each time step of the input sequence.
            hidden_size (int): The number of features in the hidden state of the LSTM.
            num_layers (int): The number of recurrent layers.
            dropout_rate (float): The dropout probability for regularization.
        """
        super(LSTMAnomalyModel, self).__init__()
        # LSTM layer processes the input sequence
        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_rate)
        # Fully connected layer to map the LSTM's last hidden state to the predicted next data point
        self.fc = nn.Linear(hidden_size, input_size) # Output size matches input size for reconstruction

    def forward(self, x):
        """
        Performs a forward pass through the LSTM model.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, sequence_length, input_size).

        Returns:
            torch.Tensor: Predicted next data point of shape (batch_size, input_size).
        """
        # x shape: (batch_size, sequence_length, input_size)
        out, _ = self.lstm(x)
        # We only care about the output of the last time step for prediction
        out = self.fc(out[:, -1, :])
        # out shape: (batch_size, input_size)
        return out

# --- LSTM Anomaly Detector for a Single Node ---
class LSTMAnomalyDetector:
    """
    Manages the LSTM model, data preprocessing, training, and anomaly prediction
    for a single network node.
    """
    def __init__(self, node_id, input_features_count, sequence_length=30, anomaly_threshold=0.05, save_shap_plots=True, plot_dir="shap_plots"):
        """
        Initializes the LSTM Anomaly Detector.

        Args:
            node_id (str): Unique identifier for the network node.
            input_features_count (int): Total number of features expected for each data point.
            sequence_length (int): The number of past data points to consider for each prediction.
            anomaly_threshold (float): The threshold for the normalized reconstruction error
                                       above which a data point is classified as an anomaly.
            save_shap_plots (bool): Whether to save SHAP plots as image/HTML files.
            plot_dir (str): Base directory to save SHAP plots. Plots will be saved in a
                            subdirectory named after the node_id.
        """
        self.node_id = node_id
        self.input_features_count = input_features_count
        self.sequence_length = sequence_length
        self.anomaly_threshold = anomaly_threshold # Tunable threshold for anomaly detection
        self.model = None
        self.scaler = MinMaxScaler() # Scaler for normalizing input features
        # Using deque for efficient rolling window for input data
        self.data_buffer = deque(maxlen=sequence_length)
        self.shap_explainer = None # SHAP explainer for model interpretability
        self.is_trained = False # Flag to indicate if the model has been trained
        self.feature_names = None # To store feature names for SHAP explanation mapping
        self.save_shap_plots = save_shap_plots
        self.plot_dir = os.path.join(plot_dir, node_id) # Plots saved per node

        if self.save_shap_plots:
            os.makedirs(self.plot_dir, exist_ok=True) # Ensure directory exists

    def build_model(self, hidden_size=64, learning_rate=0.001, num_layers=2, dropout_rate=0.2):
        """
        Builds and compiles the LSTM model with specified hyperparameters.

        Args:
            hidden_size (int): Number of hidden units in LSTM layers.
            learning_rate (float): Learning rate for the Adam optimizer.
            num_layers (int): Number of LSTM layers.
            dropout_rate (float): Dropout rate for LSTM layers.
        """
        # Ensure input_features_count is correctly set before building model
        if self.input_features_count == 0:
            raise ValueError("input_features_count must be set before building the model.")

        self.model = LSTMAnomalyModel(
            input_size=self.input_features_count,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout_rate=dropout_rate
        )
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        self.criterion = nn.MSELoss() # Mean Squared Error loss for reconstruction
        print(f"Node {self.node_id}: LSTM model built with hidden_size={hidden_size}, lr={learning_rate}, layers={num_layers}")

    def train_model(self, X_train, y_train, epochs=50, batch_size=32, verbose=0):
        """
        Trains the LSTM model using the provided training data.

        Args:
            X_train (np.array): Training input sequences (batch_size, sequence_length, n_features).
            y_train (np.array): Target output (next data point) (batch_size, n_features).
            epochs (int): Number of training epochs.
            batch_size (int): Batch size for training (currently not explicitly used with full dataset).
            verbose (int): Verbosity level (0 for silent, >0 for progress).
        """
        # Fit scaler on the entire training dataset (flattened) to learn min/max for all features
        flat_X_train = X_train.reshape(-1, self.input_features_count)
        self.scaler.fit(flat_X_train)

        # Transform both X and y using the fitted scaler
        X_train_scaled = torch.tensor(
            np.array([self.scaler.transform(seq) for seq in X_train]),
            dtype=torch.float32
        )
        y_train_scaled = torch.tensor(
            self.scaler.transform(y_train),
            dtype=torch.float32
        )

        print(f"Node {self.node_id}: Training LSTM...")
        self.model.train() # Set model to training mode
        for epoch in range(epochs):
            self.optimizer.zero_grad() # Clear gradients
            output = self.model(X_train_scaled) # Forward pass
            loss = self.criterion(output, y_train_scaled) # Calculate loss
            loss.backward() # Backward pass
            self.optimizer.step() # Update weights
            if verbose > 0 and (epoch + 1) % 10 == 0:
                print(f"Node {self.node_id}: Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}")

        self.is_trained = True
        print(f"Node {self.node_id}: Training complete. Last loss: {loss.item():.4f}")

        # Initialize SHAP explainer after model training
        # Use a subset of scaled training data as background for DeepExplainer
        self.model.eval() # Set model to evaluation mode for consistent SHAP
        background_data = X_train_scaled[np.random.choice(X_train_scaled.shape[0], min(100, X_train_scaled.shape[0]), replace=False)]
        self.shap_explainer = shap.DeepExplainer(self.model, background_data)
        self.model.train() # Set back to train mode if further training steps are expected


    def preprocess_input(self, new_data_point_raw, feature_names):
        """
        Adds a new raw data point to the internal buffer and scales the current sequence.

        Args:
            new_data_point_raw (dict): A dictionary of raw feature values for a single timestamp.
            feature_names (list): List of all feature names in their expected order.

        Returns:
            np.array or None: The scaled input sequence if buffer is full, otherwise None.
        """
        if self.feature_names is None:
            self.feature_names = feature_names # Store feature names for SHAP explanation

        # Convert raw dict to ordered numpy array for consistent feature order
        # Assumes categorical/time features are already numerically encoded (e.g., one-hot, cyclical).
        numerical_input = np.array([new_data_point_raw.get(f, 0.0) for f in feature_names], dtype=np.float32)

        # Add the current data point to the buffer
        self.data_buffer.append(numerical_input)

        if len(self.data_buffer) < self.sequence_length:
            return None # Not enough data for a full sequence yet

        # Convert deque to numpy array for scaling
        current_sequence = np.array(list(self.data_buffer))

        # Scale the current sequence using the *fitted* scaler
        # The scaler expects 2D input (n_samples, n_features)
        scaled_sequence = self.scaler.transform(current_sequence.reshape(-1, self.input_features_count))
        
        # Reshape back to (sequence_length, input_features_count) for LSTM input
        return scaled_sequence.reshape(self.sequence_length, self.input_features_count)

    async def generate_shap_plots(self, explanation_for_plotting, plot_suffix):
        """
        Generates and saves various SHAP plots for a single prediction.
        This function is designed to be called as an async task to avoid blocking.
        """
        if not self.save_shap_plots:
            return

        try:
            # Waterfall Plot (for single prediction)
            waterfall_path = os.path.join(self.plot_dir, f"waterfall_{plot_suffix}.png")
            # Clear previous plot if any
            plt.clf()
            shap.plots.waterfall(explanation_for_plotting, show=False) # show=False prevents opening window
            plt.title(f"Waterfall Plot - {self.node_id} - {plot_suffix}")
            plt.savefig(waterfall_path, bbox_inches='tight')
            plt.close() # Close plot to free memory
            print(f"Node {self.node_id}: Saved Waterfall plot to {waterfall_path}")
        except Exception as e:
            print(f"Node {self.node_id}: Error saving Waterfall plot: {e}")

        try:
            # Force Plot (for single prediction) - Saved as HTML for interactivity
            # Force plot requires `base_values` and `values` from the explanation object
            force_path = os.path.join(self.plot_dir, f"force_{plot_suffix}.html")
            shap.save_html(force_path, shap.force_plot(explanation_for_plotting.base_values, explanation_for_plotting.values, explanation_for_plotting.data, feature_names=explanation_for_plotting.feature_names, show=False))
            print(f"Node {self.node_id}: Saved Force plot to {force_path}")
        except Exception as e:
            print(f"Node {self.node_id}: Error saving Force plot: {e}")

        try:
            # Dependence Plot for the most impactful feature
            # Find the most impactful feature for this specific anomaly instance
            # Ensure there are values to calculate max
            if explanation_for_plotting.values is not None and len(explanation_for_plotting.values) > 0:
                most_impactful_feature_idx = np.argmax(np.abs(explanation_for_plotting.values))
                most_impactful_feature_name = explanation_for_plotting.feature_names[most_impactful_feature_idx]
                
                dependence_path = os.path.join(self.plot_dir, f"dependence_{most_impactful_feature_name}_{plot_suffix}.png")
                plt.clf()
                # For scatter plot, need the Explanation object for slicing
                shap.plots.scatter(explanation_for_plotting[:, most_impactful_feature_name], show=False)
                plt.title(f"Dependence Plot - {self.node_id} - {most_impactful_feature_name}")
                plt.savefig(dependence_path, bbox_inches='tight')
                plt.close()
                print(f"Node {self.node_id}: Saved Dependence plot for {most_impactful_feature_name} to {dependence_path}")
            else:
                print(f"Node {self.node_id}: Skipping Dependence plot: No SHAP values to determine impactful feature.")
        except Exception as e:
            print(f"Node {self.node_id}: Error saving Dependence plot: {e}")


    async def predict_anomaly(self, input_data_raw, all_feature_names):
        """
        Predicts anomaly score for a new incoming data point.
        Triggers SHAP explanation if an anomaly is detected.

        Args:
            input_data_raw (dict): A dictionary representing a single data point.
            all_feature_names (list): The ordered list of all features used for scaling and model input.

        Returns:
            dict: A dictionary containing anomaly detection results, including SHAP explanation if an anomaly is found.
        """
        if not self.is_trained:
            return {
                "anomaly_score": 0.0, "confidence_level": 0.0, "affected_components": [],
                "severity_classification": "N/A", "time_to_failure": "N/A",
                "root_cause_indicators": [], "recommended_actions": [],
                "shap_explanation": None,
                "status": "Model not trained"
            }

        processed_sequence = self.preprocess_input(input_data_raw, all_feature_names)

        if processed_sequence is None:
            return {
                "anomaly_score": 0.0, "confidence_level": 0.0, "affected_components": [],
                "severity_classification": "N/A", "time_to_failure": "N/A",
                "root_cause_indicators": [], "recommended_actions": [],
                "shap_explanation": None,
                "status": "Insufficient data for prediction"
            }

        # Reshape for LSTM input: (1, sequence_length, n_features)
        input_for_prediction = torch.tensor(processed_sequence, dtype=torch.float32).unsqueeze(0)

        self.model.eval() # Set model to evaluation mode
        with torch.no_grad(): # Disable gradient calculation for inference
            predicted_next_point_scaled = self.model(input_for_prediction)

        # Inverse transform predicted values and the actual last point to original scale for error calculation
        predicted_data_point_original = self.scaler.inverse_transform(predicted_next_point_scaled.cpu().numpy())
        actual_last_data_point_original = self.scaler.inverse_transform(processed_sequence[-1].reshape(1, -1))

        # Calculate reconstruction error (e.g., Mean Squared Error)
        reconstruction_error = np.mean(np.square(actual_last_data_point_original - predicted_data_point_original))
        
        # Normalize error to get anomaly score between 0 and 1
        # The `conceptual_max_error` should be determined empirically from normal data.
        # If many anomalies are showing score 1.0, this value might need tuning.
        conceptual_max_error = 0.5 # Placeholder: Needs proper tuning based on data distribution
        anomaly_score = min(1.0, reconstruction_error / conceptual_max_error)

        confidence_level = 100 * (1 - anomaly_score) # Simple inverse relationship

        is_anomaly = anomaly_score > self.anomaly_threshold

        affected_components = []
        root_cause_indicators = []
        severity_classification = "N/A"
        time_to_failure = "N/A"
        recommended_actions = []
        shap_values_output = None # Store SHAP values as data, not plots

        if is_anomaly:
            print(f"Node {self.node_id}: Anomaly detected with score: {anomaly_score:.4f}. Generating SHAP explanation...")
            try:
                self.model.eval() # Ensure model is in eval mode for SHAP explanation
                
                # Adding check_additivity=False to suppress the "do not sum up" error
                # This returns the raw SHAP output from DeepExplainer.
                raw_shap_output_from_explainer = self.shap_explainer.shap_values(input_for_prediction, check_additivity=False)
                
                # --- REVISED SHAP OUTPUT FORMAT HANDLING TO ENSURE 3D ARRAY ---
                # The goal is to get a 3D NumPy array of shape (batch_size, sequence_length, num_features)
                # regardless of how DeepExplainer returns it for a single-output LSTM.

                final_shap_3d_array = None

                if isinstance(raw_shap_output_from_explainer, np.ndarray):
                    # If it's a direct numpy array
                    if raw_shap_output_from_explainer.ndim == 4:
                        # Common for multi-output models where first dim is output_index
                        # For single-output reconstruction, it's typically (num_output_features, batch_size, seq_len, num_input_features)
                        # We need to average across the `num_output_features` dimension if we want one explanation for the whole reconstruction.
                        final_shap_3d_array = np.mean(raw_shap_output_from_explainer, axis=0) # Average over the first (output features) dimension
                    elif raw_shap_output_from_explainer.ndim == 3:
                        # This is the desired (batch_size, sequence_length, num_features) format
                        final_shap_3d_array = raw_shap_output_from_explainer
                    elif raw_shap_output_from_explainer.ndim == 2:
                        # If it's (batch_size, num_features), expand to (batch_size, 1, num_features)
                        final_shap_3d_array = raw_shap_output_from_explainer.reshape(
                            raw_shap_output_from_explainer.shape[0], 1, -1)
                    else:
                        print(f"Node {self.node_id}: SHAP output numpy array has unexpected dimensions: {raw_shap_output_from_explainer.ndim}. Expected 2, 3 or 4.")

                elif isinstance(raw_shap_output_from_explainer, list) and len(raw_shap_output_from_explainer) > 0:
                    # If it's a list (e.g., for multi-output models, containing arrays).
                    # For a single-output LSTM, the list should ideally contain one array.
                    # We iterate and stack/average to get one unified 3D array if multiple output explanations exist.
                    processed_elements = []
                    for element_shap in raw_shap_output_from_explainer:
                        if isinstance(element_shap, np.ndarray):
                            if element_shap.ndim == 3:
                                processed_elements.append(element_shap)
                            elif element_shap.ndim == 2:
                                processed_elements.append(element_shap.reshape(element_shap.shape[0], 1, -1))
                            else:
                                print(f"Node {self.node_id}: Warning: Skipping SHAP list element with unexpected dimensions: {element_shap.ndim}")
                        else:
                            print(f"Node {self.node_id}: Warning: Skipping non-numpy array in SHAP output list: {type(element_shap)}")
                    
                    if processed_elements:
                        # Stack all processed 3D elements and average them if there were multiple output explanations
                        final_shap_3d_array = np.mean(np.stack(processed_elements), axis=0)
                    else:
                        print(f"Node {self.node_id}: SHAP output list processed to be empty or contained invalid elements.")

                else:
                    print(f"Node {self.node_id}: SHAP values raw format is unknown: {type(raw_shap_output_from_explainer)}")

                # --- END OF REVISED SHAP OUTPUT FORMAT HANDLING ---


                # Proceed only if we successfully obtained valid final_shap_3d_array
                if final_shap_3d_array is not None and final_shap_3d_array.shape[1] > 0: # Ensure sequence_length dim is not 0
                    # For LSTM, we explain the impact of the INPUT SEQUENCE on the prediction.
                    # We usually focus on the impact of the LAST TIMESTEP in the input sequence.
                    # final_shap_3d_array is (batch_size, sequence_length, num_features)
                    
                    # Extract SHAP values for the last timestep (index -1)
                    # For batch_size=1, this is final_shap_3d_array[0, -1, :]
                    shap_values_for_last_timestep = final_shap_3d_array[0, -1, :]
                    
                    # Ensure base_values is also handled correctly.
                    # self.shap_explainer.expected_value can be a scalar or an array if multi-output.
                    base_values_avg = np.mean(self.shap_explainer.expected_value) if isinstance(self.shap_explainer.expected_value, np.ndarray) else self.shap_explainer.expected_value

                    explanation_for_plotting = shap.Explanation(
                        values=shap_values_for_last_timestep,
                        base_values=base_values_avg,
                        data=processed_sequence[-1], # The actual data point at the last timestep that was input
                        feature_names=self.feature_names
                    )

                    # Generate and save SHAP plots asynchronously
                    plot_suffix = f"{int(time.time())}"
                    asyncio.create_task(self.generate_shap_plots(explanation_for_plotting, plot_suffix))

                    # For root cause indicators, use absolute magnitudes
                    sorted_features_by_impact = sorted(zip(self.feature_names, shap_values_for_last_timestep), key=lambda x: np.abs(x[1]), reverse=True)

                    root_cause_indicators = []
                    affected_components = []

                    top_n = 5
                    for feature, importance in sorted_features_by_impact[:top_n]:
                        root_cause_indicators.append({"feature": feature, "importance": f"{importance:.4f}"})
                        # Map features to components based on your system's domain knowledge
                        if 'Throughput' in feature or 'Latency' in feature or 'Packet_Loss' in feature or 'Signal_Strength' in feature or 'Neighbor_Count' in feature or 'Traffic_Distribution' in feature:
                            if "Network" not in affected_components: affected_components.append("Network")
                        elif 'CPU_Utilization' in feature or 'Memory_Usage' in feature or 'Buffer_Occupancy' in feature or 'Error_Counters' in feature:
                            if "Equipment" not in affected_components: affected_components.append("Equipment")
                        elif 'Temperature' in feature or 'Power_Quality' in feature or 'Weather_Condition' in feature:
                            if "Environmental" not in affected_components: affected_components.append("Environmental")
                        elif 'Link_Status' in feature:
                            if "Connectivity" not in affected_components: affected_components.append("Connectivity")
                        elif 'Service_Priority' in feature:
                            if "Service_Configuration" not in affected_components: affected_components.append("Service_Configuration")

                    # Determine severity based on anomaly score and affected components
                    if anomaly_score > 0.75 and ("Network" in affected_components or "Equipment" in affected_components):
                        severity_classification = "Critical"
                    elif anomaly_score > 0.5:
                        severity_classification = "High"
                    elif anomaly_score > 0.25:
                        severity_classification = "Medium"
                    else:
                        severity_classification = "Low"

                    # Time to Failure and Recommended Actions (simplified based on severity)
                    if severity_classification == "Critical":
                        time_to_failure = "Immediate (0-5 min)"
                        recommended_actions.append("Initiate emergency failover/redundancy switch.")
                        recommended_actions.append("Isolate affected node for deep diagnostics.")
                    elif severity_classification == "High":
                        time_to_failure = "Short (5-30 min)"
                        recommended_actions.append("Execute automated diagnostics and health checks.")
                        recommended_actions.append("Prepare for traffic rerouting/load shedding.")
                    elif severity_classification == "Medium":
                        time_to_failure = "Medium (30-120 min)"
                        recommended_actions.append("Monitor closely and review recent configuration changes.")
                        recommended_actions.append("Schedule proactive maintenance if trend continues.")
                    else:
                        time_to_failure = "Long (>120 min)"
                        recommended_actions.append("Log for trend analysis.")
                        recommended_actions.append("No immediate action required.")

                    shap_values_output = {
                        "feature_names": self.feature_names,
                        "importances": shap_values_for_last_timestep.tolist(),
                        "raw_shap_values_for_last_timestep": final_shap_3d_array[0, -1, :].tolist()
                    }

                else:
                    print(f"Node {self.node_id}: Final SHAP values array is invalid or empty. Cannot generate detailed explanation.")
                    root_cause_indicators = [{"error": "SHAP explanation failed", "details": "SHAP values array invalid or empty after processing."}]
                    affected_components = ["Unknown"]
                    severity_classification = "Low"
                    recommended_actions = ["Investigate manually."]
                    shap_values_output = None

            except Exception as e:
                print(f"Node {self.node_id}: SHAP explanation failed: {e}")
                root_cause_indicators = [{"error": "SHAP explanation failed", "details": str(e)}]
                affected_components = ["Unknown"]
                severity_classification = "Low"
                recommended_actions = ["Investigate manually."]
                shap_values_output = None

        return {
            "anomaly_score": round(anomaly_score, 4),
            "confidence_level": round(confidence_level, 2),
            "affected_components": affected_components,
            "severity_classification": severity_classification,
            "time_to_failure": time_to_failure,
            "root_cause_indicators": root_cause_indicators,
            "recommended_actions": recommended_actions,
            "shap_explanation": shap_values_output,
            "status": "Anomaly detected" if is_anomaly else "Normal"
        }

# --- Calculation Agent (Main Class) ---
class CalculationAgent:
    """
    The main Calculation Agent responsible for managing anomaly detectors for multiple nodes,
    receiving data, triggering predictions, and communicating with other agents (Healing, MCP).
    """
    def __init__(self, node_ids, pub_socket_address_a2a, push_socket_address_mcp, sequence_length=30):
        """
        Initializes the Calculation Agent.

        Args:
            node_ids (list): A list of unique identifiers for all network nodes to monitor.
            pub_socket_address_a2a (str): ZeroMQ address for publishing A2A anomaly alerts to the Healing Agent.
            push_socket_address_mcp (str): ZeroMQ address for pushing status updates to the MCP.
            sequence_length (int): The length of the time series sequence for LSTM.
        """
        self.node_ids = node_ids
        self.pub_socket_address_a2a = pub_socket_address_a2a
        self.push_socket_address_mcp = push_socket_address_mcp

        self.sequence_length = sequence_length
        self.node_detectors = {} # Stores LSTMAnomalyDetector instances per node
        
        # Initialize these as None/0; they will be set dynamically after loading the dataset
        self.all_features = None
        self.input_features_count = 0 

        # _initialize_detectors now just creates detector objects, model building is moved to start()
        self._initialize_detectors() 
        self.context = zmq.asyncio.Context()
        
        # A2A communication: PUB socket to publish anomaly alerts to Healing Agent
        self.a2a_publisher_socket = self.context.socket(zmq.PUB)
        self.a2a_publisher_socket.bind(self.pub_socket_address_a2a)
        print(f"Calculation Agent: A2A Publisher bound to {self.pub_socket_address_a2a}")

        # MCP communication: PUSH socket to send status updates to MCP
        self.mcp_push_socket = self.context.socket(zmq.PUSH)
        self.mcp_push_socket.connect(self.push_socket_address_mcp) # Connect to MCP's PULL socket
        print(f"Calculation Agent: MCP PUSH connected to {self.mcp_push_socket}")


    def _initialize_detectors(self):
        """
        Initializes an LSTM anomaly detector instance for each configured node.
        Model building is delayed until input_features_count is known.
        """
        for node_id in self.node_ids:
            self.node_detectors[node_id] = LSTMAnomalyDetector(
                node_id=node_id,
                input_features_count=0, # Placeholder, will be updated before model build
                sequence_length=self.sequence_length
            )
            # build_model() is NOT called here anymore. It's called in start() after data load.

    async def _objective_function(self, params, node_id, X_train, y_train):
        """
        Objective function for SSA to optimize LSTM hyperparameters.
        This function is called by the SalpSwarmOptimizer.
        It evaluates a temporary LSTM model with given hyperparameters and returns its loss.

        Args:
            params (np.array): Array of hyperparameters (hidden_size, learning_rate, num_layers, dropout_rate).
            node_id (str): The ID of the node for which to optimize.
            X_train (np.array): Training input data.
            y_train (np.array): Training target data.

        Returns:
            float: The loss (fitness) of the temporary model.
        """
        hidden_size, learning_rate, num_layers, dropout_rate = \
            int(params[0]), float(params[1]), int(params[2]), float(params[3])
        
        # Ensure hyperparameters are within valid and reasonable ranges
        hidden_size = max(16, min(hidden_size, 256))
        learning_rate = max(1e-5, min(learning_rate, 1e-2))
        num_layers = max(1, min(num_layers, 5))
        dropout_rate = max(0.0, min(dropout_rate, 0.5))

        # Create a temporary detector for evaluation by SSA
        temp_detector = LSTMAnomalyDetector(
            node_id=f"temp_{node_id}", # Use a temporary ID
            input_features_count=self.input_features_count, # Use the already determined count
            sequence_length=self.sequence_length
        )
        temp_detector.build_model(
            hidden_size=hidden_size,
            learning_rate=learning_rate,
            num_layers=num_layers,
            dropout_rate=dropout_rate
        )

        try:
            # Train the temporary model for a few epochs to get an estimate of performance
            temp_detector.train_model(X_train, y_train, epochs=5, verbose=0)
            
            # Evaluate the model on the training data to get a loss score
            temp_detector.model.eval() # Set to evaluation mode
            with torch.no_grad(): # Disable gradient calculation
                # Scale X_train before passing to model for evaluation
                X_train_scaled_eval = torch.tensor(
                    temp_detector.scaler.transform(X_train.reshape(-1, self.input_features_count)).reshape(X_train.shape),
                    dtype=torch.float32
                )
                # Scale y_train before passing to criterion
                y_train_scaled_eval = torch.tensor(
                    temp_detector.scaler.transform(y_train),
                    dtype=torch.float32
                )
                output = temp_detector.model(X_train_scaled_eval)
                loss = temp_detector.criterion(output, y_train_scaled_eval).item()
        except Exception as e:
            print(f"Node {node_id}: Error during SSA objective function: {e}")
            loss = float('inf') # Penalize errors heavily during optimization

        return loss

    async def optimize_lstm_with_ssa(self, node_id, X_train_data, y_train_data):
        """
        Optimizes LSTM hyperparameters for a specific node using the Salp Swarm Algorithm (SSA).
        After optimization, the node's main detector is rebuilt and retrained with the best parameters.

        Args:
            node_id (str): The ID of the node whose detector hyperparameters are to be optimized.
            X_train_data (np.array): Training input data for the node.
            y_train_data (np.array): Training target data for the node.
        """
        print(f"Node {node_id}: Starting SSA optimization for LSTM hyperparameters...")
        # Define search space for hyperparameters: [hidden_size, learning_rate, num_layers, dropout_rate]
        lower_bounds = [32, 0.0001, 1, 0.0]
        upper_bounds = [128, 0.005, 3, 0.3]

        # Use functools.partial to pass fixed arguments to the objective function
        obj_func_with_data = partial(self._objective_function, node_id=node_id, X_train=X_train_data, y_train=y_train_data)

        ssa = SalpSwarmOptimizer(
            obj_func=obj_func_with_data,
            n_salps=5,       # Number of salps (population size) - reduced for faster demo
            n_iterations=3,  # Number of iterations for SSA - reduced for faster demo
            lower_bounds=lower_bounds,
            upper_bounds=upper_bounds
        )

        best_params, best_loss = await ssa.optimize()
        print(f"Node {node_id}: SSA Optimization complete. Best params: {best_params}, Best Loss: {best_loss:.4f}")

        # Extract optimized hyperparameters
        optimized_hidden_size = int(best_params[0])
        optimized_lr = float(best_params[1])
        optimized_num_layers = int(best_params[2])
        optimized_dropout = float(best_params[3])

        # Update the node's main detector with optimized parameters and retrain fully
        detector = self.node_detectors[node_id]
        detector.build_model(
            hidden_size=optimized_hidden_size,
            learning_rate=optimized_lr,
            num_layers=optimized_num_layers,
            dropout_rate=optimized_dropout
        )
        detector.train_model(X_train_data, y_train_data, epochs=100) # Increased epochs for full retraining
        print(f"Node {node_id}: LSTM model updated with optimized parameters and fully retrained.")


    def load_dataset(self, file_path):
        """
        Loads the entire dataset from a CSV file.
        Ensures all expected features are present and in the correct order.
        """
        print(f"Calculation Agent: Loading dataset from {file_path}...")
        try:
            df = pd.read_csv(file_path)
            
            # This part will be executed after self.all_features is set in start()
            if self.all_features:
                # Ensure all expected features are in the dataframe, fill with zeros if missing
                for feature in self.all_features:
                    if feature not in df.columns:
                        print(f"Warning: Feature '{feature}' not found in dataset. Filling with zeros.")
                        df[feature] = 0.0

                # Ensure the order of columns matches self.all_features
                df = df[self.all_features]
            else:
                # This case should ideally not be hit if start() calls load_dataset first.
                # If it is, it means all_features wasn't set, so we just return the full df.
                # The filtering of non-feature columns should happen in start()
                pass
                
            print(f"Calculation Agent: Dataset loaded with {len(df)} records and {len(df.columns)} original features.")
            return df
        except FileNotFoundError:
            print(f"Error: Dataset file not found at {file_path}. Please ensure it's in the correct directory.")
            sys.exit(1)
        except Exception as e:
            print(f"Error loading dataset: {e}")
            sys.exit(1)

    async def process_data_point(self, node_id, raw_data_point_dict): # Renamed from process_data_stream
        """
        Receives a single raw data point for a node, processes it,
        triggers anomaly detection, and sends alerts/status updates.
        This method is called for each data point from the dataset.
        """
        detector = self.node_detectors.get(node_id)
        if not detector:
            print(f"Error: No detector found for node {node_id}. This should not happen if initialized properly.")
            return

        if not detector.is_trained:
            print(f"Node {node_id}: Detector not trained. Skipping processing this data point.")
            return # Skip if not trained, training is handled in start()

        print(f"Node {node_id}: Processing data at {raw_data_point_dict.get('timestamp', 'N/A')}")
        # Pass self.all_features which is now correctly set from loaded data
        anomaly_results = await detector.predict_anomaly(raw_data_point_dict, self.all_features)

        # A2A Communication: Publish anomaly alerts to the Healing Agent (PUB/SUB pattern)
        a2a_message = {
            "source": "CalculationAgent",
            "type": "anomaly_alert",
            "node_id": node_id,
            "timestamp": raw_data_point_dict.get('timestamp', time.time()),
            "anomaly_data": anomaly_results
        }
        await self.a2a_publisher_socket.send_json(a2a_message)
        print(f"Node {node_id}: A2A: Published anomaly alert (Status: {anomaly_results['status']}, Score: {anomaly_results['anomaly_score']:.4f})")

        # MCP Communication: Push status update to MCP (PUSH/PULL pattern)
        mcp_message = {
            "source": "CalculationAgent",
            "type": "status_update",
            "node_id": node_id,
            "timestamp": raw_data_point_dict.get('timestamp', time.time()),
            "status": anomaly_results["status"],
            "details": anomaly_results # Send full anomaly details to MCP for logging/dashboard
        }
        await self.mcp_push_socket.send_json(mcp_message)
        print(f"Node {node_id}: MCP: Pushed status update.")


    async def start(self):
        """
        Starts the Calculation Agent, loading data from a real dataset
        and processing them for anomaly detection.
        """
        print("Calculation Agent started. Loading and processing data from real dataset...")
        
        # Load the entire dataset first to determine features
        # The load_dataset method will return the dataframe with all columns from the CSV.
        temp_full_dataset_df = self.load_dataset(DATASET_PATH) # Load initially without feature subsetting
        
        # --- IMPORTANT: Set self.all_features and self.input_features_count based on loaded data ---
        # Exclude 'timestamp', 'node_id', and 'is_anomaly' (if present) as they are metadata/labels, not model features
        model_features = [col for col in temp_full_dataset_df.columns if col not in ['timestamp', 'node_id', 'is_anomaly']]
        self.all_features = model_features
        self.input_features_count = len(self.all_features)
        print(f"Calculation Agent: Detected {self.input_features_count} features: {self.all_features}")
        
        # Now, subset the full_dataset_df to only include the features for the model
        self.full_dataset_df = temp_full_dataset_df[self.all_features]

        # Now that input_features_count and all_features are known, build models and prepare detectors
        for node_id in self.node_ids:
            # Update the existing detector instance with correct input_features_count
            detector = self.node_detectors[node_id] 
            detector.input_features_count = self.input_features_count
            detector.build_model() # Build model with correct input size
            detector.feature_names = self.all_features # Ensure feature names are set for SHAP

        # Initial training for all detectors if not trained yet
        for node_id in self.node_ids: # This loop now runs *after* models are built
            detector = self.node_detectors.get(node_id)
            if detector and not detector.is_trained:
                print(f"Node {node_id}: Detector not trained. Using initial data from dataset for training/optimization.")
                
                # Use a portion of the dataset for initial training.
                # Ensure you have enough data in your CSV for this.
                initial_training_size = self.sequence_length * 10 # Example: 10x sequence length for training
                if len(self.full_dataset_df) < initial_training_size + self.sequence_length:
                    print(f"Warning: Dataset too small for initial training. Minimum {initial_training_size + self.sequence_length} rows needed. Using all available data.")
                    initial_training_size = len(self.full_dataset_df) - self.sequence_length
                    if initial_training_size <= 0:
                        print("Error: Dataset too small even for one sequence. Cannot train. Please ensure training_data.csv has enough rows.")
                        sys.exit(1)

                initial_training_data_df = self.full_dataset_df.head(initial_training_size + self.sequence_length)
                
                X_train_list, y_train_list = [], []
                # Iterate up to initial_training_size to form sequences
                for i in range(initial_training_size):
                    sequence_df = initial_training_data_df.iloc[i : i + self.sequence_length]
                    target_df = initial_training_data_df.iloc[i + self.sequence_length] # The point to predict

                    # Ensure we only take the actual features, not metadata/labels
                    X_train_list.append(sequence_df.values) # Already subsetted to self.all_features
                    y_train_list.append(target_df.values)   # Already subsetted to self.all_features

                X_mock = np.array(X_train_list, dtype=np.float32)
                y_mock = np.array(y_train_list, dtype=np.float32)

                detector.train_model(X_mock, y_mock, epochs=100, verbose=0) # Increased epochs for initial training
                await self.optimize_lstm_with_ssa(node_id, X_mock, y_mock)
                
        self.data_iterator = temp_full_dataset_df.iterrows() # Reset iterator to original full DataFrame


        print("Calculation Agent: Processing data stream from real dataset...")
        # Main loop to process data points from the dataset
        while True:
            try:
                # Get the next data point from the dataset
                # We'll just assign all data to node_1 for this simple example.
                # In a multi-node dataset, you'd extract node_id from the row.
                index, row_series = next(self.data_iterator)
                raw_data_point_dict = row_series.to_dict()
                
                # Add a timestamp if your CSV doesn't have one, or use existing
                if 'timestamp' not in raw_data_point_dict:
                    raw_data_point_dict['timestamp'] = time.time() # Or use a time from your CSV if available

                # For simplicity, assign all data to node_1 for now.
                # If your CSV has a 'node_id' column, use row_series['node_id']
                node_to_process = f"node_1" # Or retrieve from raw_data_point_dict if available

                # Ensure the node exists in our detectors list (it should from _initialize_detectors)
                if node_to_process not in self.node_detectors:
                    print(f"Warning: Data for unknown node {node_to_process}. Skipping.")
                    continue

                # Process the data point (anomaly detection, alerting, MCP update)
                asyncio.create_task(self.process_data_point(node_to_process, raw_data_point_dict))
                
                # Adjust sleep based on your dataset's time intervals or desired simulation speed
                await asyncio.sleep(0.1) # Simulate real-time delay (e.g., 100ms per data point)

            except StopIteration:
                print("Calculation Agent: End of dataset reached. Looping back to beginning.")
                self.data_iterator = temp_full_dataset_df.iterrows() # Loop dataset
                await asyncio.sleep(1) # Small delay before restarting iteration
            except Exception as e:
                print(f"Calculation Agent: Unhandled error in data stream loop: {e}")
                # Log the error and continue or implement more robust error handling
                await asyncio.sleep(1)


if __name__ == "__main__":
    # Standalone execution for Calculation Agent (for testing purposes)
    print("Running standalone test for Calculation Agent...")

    # Define ZeroMQ addresses for standalone testing
    # These should match the addresses expected by Healing Agent and MCP Agent
    test_pub_address_a2a = "tcp://127.0.0.1:5556"
    test_push_address_mcp = "tcp://127.0.0.1:5557"

    test_node_ids = [f"node_1"] # Assuming your synthetic dataset is for one node

    # Initialize and start the Calculation Agent
    calc_agent = CalculationAgent(
        node_ids=test_node_ids,
        pub_socket_address_a2a=test_pub_address_a2a,
        push_socket_address_mcp=test_push_address_mcp
    )

    try:
        asyncio.run(calc_agent.start())
    except KeyboardInterrupt:
        print("\nCalculation Agent standalone test stopped.")
        sys.exit(0)
    except Exception as e:
        print(f"An error occurred during standalone Calculation Agent run: {e}")
        sys.exit(1)